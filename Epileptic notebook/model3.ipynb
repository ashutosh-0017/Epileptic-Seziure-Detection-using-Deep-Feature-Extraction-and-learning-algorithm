{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563f83d2-5f7e-40b2-a874-6f838343dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Required Libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import scipy\n",
    "import sklearn\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc20645-00ee-45ae-9d2e-ca9f8dddf384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in eeg-predictive_train.npz: ['train_signals', 'train_labels']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load EEG Data\n",
    "data_folder = r\"D:\\dataset eeg\"\n",
    "\n",
    "# Get all .npz files in the folder\n",
    "npz_files = [f for f in os.listdir(data_folder) if f.endswith('.npz')]\n",
    "\n",
    "# Load all .npz files into a dictionary\n",
    "eeg_data = {}\n",
    "for file in npz_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    eeg_data[file] = data\n",
    "\n",
    "# Print keys of the first file\n",
    "first_file = npz_files[0]\n",
    "print(f\"Keys in {first_file}: {list(eeg_data[first_file].keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145c8275-f857-49e0-b0d0-5144654b0185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Train Signals Shape: (45948, 23, 256)\n",
      "Merged Train Labels Shape: (45948,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Merge Training Data\n",
    "train_signals_1 = eeg_data[\"eeg-predictive_train.npz\"][\"train_signals\"]\n",
    "train_labels_1 = eeg_data[\"eeg-predictive_train.npz\"][\"train_labels\"]\n",
    "\n",
    "train_signals_2 = eeg_data[\"eeg-seizure_train.npz\"][\"train_signals\"]\n",
    "train_labels_2 = eeg_data[\"eeg-seizure_train.npz\"][\"train_labels\"]\n",
    "\n",
    "# Merge the datasets\n",
    "train_signals = np.concatenate((train_signals_1, train_signals_2), axis=0)\n",
    "train_labels = np.concatenate((train_labels_1, train_labels_2), axis=0)\n",
    "\n",
    "# Print shape of merged data\n",
    "print(\"Merged Train Signals Shape:\", train_signals.shape)\n",
    "print(\"Merged Train Labels Shape:\", train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00db6bf-3f25-499f-ad58-4cbeeadc3a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Train Signals Shape: (10000, 23, 256)\n",
      "Balanced Train Labels Shape: (10000,)\n",
      "New Label Distribution: {0.0: 5000, 1.0: 5000}\n"
     ]
    }
   ],
   "source": [
    "# Find indices of seizure (1) and non-seizure (0) samples\n",
    "seizure_indices = np.where(train_labels == 1)[0]\n",
    "non_seizure_indices = np.where(train_labels == 0)[0]\n",
    "\n",
    "# Randomly select 5,000 samples from each class\n",
    "np.random.seed(42)  # For reproducibility\n",
    "seizure_sample = np.random.choice(seizure_indices, 5000, replace=False)\n",
    "non_seizure_sample = np.random.choice(non_seizure_indices, 5000, replace=False)\n",
    "\n",
    "# Combine selected samples\n",
    "selected_indices = np.concatenate((seizure_sample, non_seizure_sample))\n",
    "\n",
    "# Subset the dataset\n",
    "train_signals_balanced = train_signals[selected_indices]\n",
    "train_labels_balanced = train_labels[selected_indices]\n",
    "\n",
    "# Print new dataset size\n",
    "print(\"Balanced Train Signals Shape:\", train_signals_balanced.shape)\n",
    "print(\"Balanced Train Labels Shape:\", train_labels_balanced.shape)\n",
    "\n",
    "# Check label distribution\n",
    "unique, counts = np.unique(train_labels_balanced, return_counts=True)\n",
    "print(\"New Label Distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed1c9f26-d1f7-4f4f-b26b-e18f014fb16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Label Distribution: {0: 5000, 1: 1000, 2: 4000}\n"
     ]
    }
   ],
   "source": [
    "# Compute Mean Absolute Value (MAV) for each sample\n",
    "mav = np.mean(np.abs(train_signals_balanced), axis=(1,2))\n",
    "\n",
    "# Define thresholds for classification\n",
    "severe_threshold = np.percentile(mav, 90)  # Top 10% = Severe\n",
    "mild_threshold = np.percentile(mav, 50)    # Middle 50-90% = Mild\n",
    "\n",
    "# Assign new labels based on amplitude\n",
    "new_labels = []\n",
    "for value in mav:\n",
    "    if value >= severe_threshold:\n",
    "        new_labels.append(1)  # Severe Epileptic\n",
    "    elif value >= mild_threshold:\n",
    "        new_labels.append(2)  # Mild Epileptic\n",
    "    else:\n",
    "        new_labels.append(0)  # Non-Epileptic\n",
    "\n",
    "train_labels_balanced = np.array(new_labels)\n",
    "\n",
    "# Check new label distribution\n",
    "unique, counts = np.unique(train_labels_balanced, return_counts=True)\n",
    "print(\"Updated Label Distribution:\", dict(zip(unique, counts)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19997d2e-4c70-4dfc-af24-900c1693739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Signals After Normalization:\n",
      "Min: 0.0 Max: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalize EEG signals (Min-Max Scaling)\n",
    "train_signals_balanced = (train_signals_balanced - np.min(train_signals_balanced)) / (np.max(train_signals_balanced) - np.min(train_signals_balanced))\n",
    "\n",
    "print(\"EEG Signals After Normalization:\")\n",
    "print(\"Min:\", np.min(train_signals_balanced), \"Max:\", np.max(train_signals_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "badce6a5-404c-45ae-9a20-b496e322e6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Signals After Bandpass Filtering:\n",
      "Shape: (10000, 23, 256)\n"
     ]
    }
   ],
   "source": [
    "# Bandpass filter function (removes noise outside 0.5–40 Hz)\n",
    "def bandpass_filter(data, lowcut=0.5, highcut=40.0, fs=256, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, data, axis=-1)\n",
    "\n",
    "# Apply bandpass filter to EEG signals\n",
    "train_signals_filtered = bandpass_filter(train_signals_balanced)\n",
    "\n",
    "print(\"EEG Signals After Bandpass Filtering:\")\n",
    "print(\"Shape:\", train_signals_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11bd53af-0ed9-4fd4-ae2c-aca461f59c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of One-Hot Encoded Labels:\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "train_labels_onehot = to_categorical(train_labels_balanced, num_classes=3)\n",
    "\n",
    "# Print first 5 samples to check encoding\n",
    "print(\"Example of One-Hot Encoded Labels:\")\n",
    "print(train_labels_onehot[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec19e2a3-0f02-4ddd-9139-32e24ef5d93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 317ms/step\n",
      "InceptionV3 Feature Shape: (10000, 1, 1024)\n",
      "Epoch 1/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 7s/step - accuracy: 0.4287 - loss: 2.8848 - val_accuracy: 0.6095 - val_loss: 0.8487\n",
      "Epoch 2/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m664s\u001b[0m 7s/step - accuracy: 0.4523 - loss: 1.0067 - val_accuracy: 0.3540 - val_loss: 0.8960\n",
      "Epoch 3/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m660s\u001b[0m 7s/step - accuracy: 0.4414 - loss: 1.0366 - val_accuracy: 0.6095 - val_loss: 0.9222\n",
      "Epoch 4/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m660s\u001b[0m 7s/step - accuracy: 0.4463 - loss: 1.0553 - val_accuracy: 0.6095 - val_loss: 0.8280\n",
      "Epoch 5/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 7s/step - accuracy: 0.4494 - loss: 0.9872 - val_accuracy: 0.3540 - val_loss: 0.8987\n",
      "Epoch 6/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m676s\u001b[0m 7s/step - accuracy: 0.4610 - loss: 0.9738 - val_accuracy: 0.6095 - val_loss: 0.8394\n",
      "Epoch 7/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 6s/step - accuracy: 0.4433 - loss: 0.9837 - val_accuracy: 0.3540 - val_loss: 0.8697\n",
      "Epoch 8/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 6s/step - accuracy: 0.4410 - loss: 1.0088 - val_accuracy: 0.6095 - val_loss: 0.9132\n",
      "Epoch 9/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 7s/step - accuracy: 0.4515 - loss: 0.9808 - val_accuracy: 0.3540 - val_loss: 0.8842\n",
      "Epoch 10/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m639s\u001b[0m 6s/step - accuracy: 0.4584 - loss: 0.9897 - val_accuracy: 0.6095 - val_loss: 0.7997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x188392f3980>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Extraction using InceptionV3 (adapted for time series)\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Reshape, Resizing, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Reshape input to (samples, time_steps, channels)\n",
    "train_signals_inception = np.transpose(train_signals_filtered, (0, 2, 1))\n",
    "\n",
    "# Add a dummy height dimension to make it compatible with InceptionV3\n",
    "train_signals_inception = np.expand_dims(train_signals_inception, axis=-1)  # Add channel dimension\n",
    "train_signals_inception = np.repeat(train_signals_inception, 3, axis=-1) #repeat to make 3 channels.\n",
    "\n",
    "# Resize the second dimension to at least 75\n",
    "target_width = 75\n",
    "train_signals_inception = Resizing(height=train_signals_inception.shape[1], width=target_width)(train_signals_inception)\n",
    "\n",
    "# Load InceptionV3 (without top layers)\n",
    "base_model = InceptionV3(weights=None, include_top=False, input_shape=(train_signals_inception.shape[1], target_width, 3))\n",
    "\n",
    "# Add custom layers for time series\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x) #Reduce feature dimension.\n",
    "x = Reshape((1, -1))(x)\n",
    "\n",
    "model_inception = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Feature extraction\n",
    "features = model_inception.predict(train_signals_inception)\n",
    "\n",
    "print(\"InceptionV3 Feature Shape:\", features.shape)\n",
    "\n",
    "# Transformer Model\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Transformer Parameters\n",
    "num_heads = 4 #reduce number of heads.\n",
    "ff_dim = 1024 #reduce ff_dim\n",
    "\n",
    "input_shape = features.shape[1:]\n",
    "\n",
    "# Transformer Layers\n",
    "inputs = Input(shape=input_shape)\n",
    "attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=features.shape[-1])(inputs, inputs)\n",
    "norm_output = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "ffn_output = Dense(ff_dim, activation='relu')(norm_output)\n",
    "ffn_output = Dense(features.shape[-1])(ffn_output)\n",
    "transformer_output = LayerNormalization(epsilon=1e-6)(norm_output + ffn_output)\n",
    "\n",
    "# Classification Layers\n",
    "flat_output = tf.keras.layers.Flatten()(transformer_output)\n",
    "outputs = Dense(3, activation='softmax')(flat_output)\n",
    "\n",
    "# Transformer Model\n",
    "transformer_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "transformer_model.fit(features, train_labels_onehot, epochs=10, batch_size=80, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b5551-5b47-4c08-aa07-4d3402060663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
